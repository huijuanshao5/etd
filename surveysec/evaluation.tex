\section{Evaluation Methodology}
\label{sec:evaluation}
To the best of our knowledge, there is no universally
accepted evaluation measure(s)
to evaluate the 
results of energy disaggregation algorithms.
In the situation where we know the true power consumption value of each device, there are two broad classes of measurements to evaluate 
device disaggregation results: event-based and time 
series-based. In an event-based measure, we check whether the 
inferred on/off events are correctly determined 
for the target device. 
The time series measure gauges whether the disaggregated 
power values of each device 
are in the range of ground truth over a period of time. 
For both event-based and time series-based measures, 
the disaggregation results can be measured through a confusion matrix, 
F-measure, or a simple error rate. 
 
%We introduce precision, recall, and F-measure to evaluate it.
%In classification, true positive (TP) means the result is correct;
%false positive (FP) means the unexpected result; 
%true negative (TN) means the missing results; 
%and false negative (FN) means the correct absent values.
%Precision is to calculate for the disaggregated energy,
%how much is correct. Recall is to calculate how much
%energy is identified.
%The precision and recall are calculated as Equation (\ref{eq_precision}) 
%and Equation (\ref{eq_recall}). 
%\begin{equation}
%\label{eq_precision}
%precision=\frac{TP}{TP+FP}
%\end{equation}
%\begin{equation}
%\label{eq_recall}
%recall=\frac{TP}{TP+FN}
%\end{equation}
%F-measure value is the harmonic
%mean of precision and recall.
%\begin{equation}
%\label{eq_fmeasure}
%F-score= 2 \times \frac{precision \times recall}{precision + recall }
%\end{equation}

%The evaluation metric includes the events classification error rate
%\cite{patel2007flick} and time point-to-point energy comparison \cite{kolter2012aistat}.

%\begin{table}
%\tbl{Evaluation Metrics \label{tb_evaluation}} {
%\begin{tabular}{|c|c|c|}
%\hline
%& events & time series\\
%\hline
%taxonomy & set classification & point-to-point \\
%\hline
%F-measure & TP,FP,TN,FN for events & TP,FP,TN,FN for each energy value \\
%\hline
%error rate & event is classified into right device & for each data point, the value is classified right \\
%\hline
%\end{tabular}}
%\end{table}
%The events includes real power changes events,
%real power and reactive identification events,
%transient shape events and so on.

\subsection{Evaluation based on Events}
Event-based evaluation measures, primarily on and off events, have been widely used in 
previous research work. 
They are identified by real power and reactive power as described in~\cite{berges2009learning} and~\cite{chang2010newmethod},
by real power and transient shapes~\cite{froehlich2011disaggregated},
by real power, reactive power and voltage-current trajectory~\cite{yang2007design},
by just transient shapes~\cite{chang2008load},
by comparing the waveforms as shown in~\cite{suzuki2008nonintrusive} and~\cite{berges2010enhancing},
or by analyzing the voltage noise~\cite{patel2007flick}.
%No matter which feature is adopted and
%which algorithm is applied,
%the target is to improve the
%event classification accuracy.

Generally, the event classification rate is
calculated using the following procedure.
For each device,
we suppose there are a total of $N$ on or off events 
$\{E_1, ..., E_i, ..., E_N\}$ during
a period of time;
assuming the corresponding predicted events are
$\{\hat{E_1}, ..., \hat{E_i},...,\hat{E_N}\}$,
then, according to ~\cite{nakano2007non}, the coverage range of these on or off events is
given in Equation (\ref{eq_eventEvaluation}).
\begin{equation}
\label{eq_eventEvaluation}
E_{coverage} = \frac{\sum_{i=1}^N (E_i - \hat{E_i})}{\sum E_i}
\end{equation}
Higher coverage for a device thus implies better prediction results.

%In order the improve the classification accuracy,
%\cite{gupta2010electrisense} and \cite{onoda2000applying}
%trains data by cross-validation.


%Equation.\ref{eq_eventEvaluation} according to devices
%over the test time period.
%This time period can be specified by users.
%\cite{kolter2010sparse} evaluates the results with one-week's
%accumulated data.
%\begin{equation}
%Accurage \equiv \frac{\sum_{i,q}\min\{\sum_p(X_i)_{pq}, \sum_p(B_i\hat{A}_i)_{pq} \}}{\sum_{p,q}\bar{X}_{p,q}}
%\end{equation}

After this calculation, the disaggregation accuracy rate
can be calculated by judging whether
the disaggregated devices are classified as the
correct device. Reference~\cite{gonccalves2011unsupervised} evaluates performance
based on this criteria, as given in Equation (\ref{eq_clusterEvaluation}).
\begin{equation}
\label{eq_clusterEvaluation}
purity_m(\Omega, C) = \frac{1}{1/N_m}\sum_k \max_m |\omega_k \subset c_m|
\end{equation}
where $\Omega$ is the set of all ground truth device labels,
and $C$ is the disaggregated device labels set.
Let us suppose there are $M$ number of clusters corresponding to $M$ devices,
$N_m$ is the number of elements in cluster $m$, 
and $\omega_k$ is the subset with the highest frequency in each $c_j=m$ cluster.
In addition to the classification accuracy rate discussed above, 
the F-measure is also employed in~\cite{zeifman2012disaggregation} 
to define the disaggregation performance.
%We hypothesize that there are
%ground truth event sets $\Omega$ and disaggregated event sets $C$.
%Under this assumption,
%for device $k$,
%true positive means an event in $C_k$ is correctly classified into $\Omega_k$.
%False positive represents that an event in $C_k$,
%which should not belong to  $\Omega_k$, is classified into $\Omega_k$.
%True negative shows that an event in $C_k$,
%should belong to $\Omega_k$, is not classified into $\Omega_k$.
%False negative means that an excessive event,
%which does not belong to $\Omega_k$
%is missed by disaggregated devices $C_k$.
%Then F-measure score can be calculated by Equation (\ref{eq_fmeasure}).


%\cite{onoda2000applying} also uses cross-validation to classify the events
%with generalized error.
%Similarly, \cite{patel2007flick} classify events.
%\citeNP{suzuki2008nonintrusive,berges2010enhancing} also evaluates ON/OFF events
%by comparing the waveforms to judge which device
%\cite{gupta2010electrisense} gets the events classification
%rate by 10 fold cross-validation training procedure.
%It says to discover two simultaneous events with more than
%102ms gaps.
%\cite{yang2007design} evaluates whether the events related to
%P-Q and V-I values are correctly classified into correct event types.
%
%\cite{yang2007design} evaluates whether the devices are classified into
%right types of devices according to V-I trajectory.
%\cite{chang2008load} calculates the correctly classified loads based on
%turn-on transient shapes.
%\cite{berges2009learning} calculates the correct rate that
%how many events are classified into loads based on real power and reactive power.
%\cite{chang2010newmethod} classify P-Q events and evaluate
%the correctly classification rate.
%\cite{froehlich2011disaggregated} classify real and transient events rate.


\subsection{Evaluation based on Time Series}
The time series approach compares the disaggregated power values with the ground 
truth power values at each point over a period of time.
For instance, ~\cite{kolter2012aistat} compare the
disaggregated time series with the ground truth of each device as:
\begin{equation}
\label{eq_tsErrorRate}
\sqrt{(\sum_{t,m} \lVert y_t^{(m)}- \hat{y}_t^{(m)} \rVert_2^2)/(\sum_{t,m} \lVert y_t^{(m)}\rVert_2^2)}
\end{equation}
where $y_t$ is the true real power value at time $t$, 
and $\hat{y}$ is the disaggregated real power value. 
Note that the disaggregate error rate can be calculated over a
specific time range~\cite{kolter2012aistat}. 
In addition,~\cite{parson2012nonintrusive} uses the square root 
of the error rate of all devices over a period of time to calculate 
the disaggregation accuracy, as shown in Equation (\ref{eq_tsEvaluationSqt}). 
\begin{equation}
\label{eq_tsEvaluationSqt}
\sqrt{\frac{1}{T}\sum_t(y_t^{(m)}-\hat{y}_{\mu_t^m}^{(m)})^2}
\end{equation}
where $m$ is the number of devices, 
$y_t^m$ is the power value of device $m$ at time $t$,  
$\mu_t^m$ is the average true power value of device $m$ at time $t$. 

%\begin{equation}
%\label{eq_tsEvaluationDays}
%|\sum_t x_t^{(n)}-\sum_t \mu_{z_t^{(n)}}^{(n)}|/ \sum_t x_t^{(n)}
%\end{equation}
%where $n$ means the

The third method to measure time series data is the
F-measure~\cite{kim2011unsupervised}.
%Suppose at time $t_i$, 
%the disaggregated value is $\hat{y}$ and 
%true value is $y$, 
%then true positive means 
%$\hat{y}$ is in the acceptable range of $y$. 
%The acceptable range can be defined by users 
%as $(0.8y, 1.2y)$ as needed. 
%True negative means at time point $t_i$, 
%the disaggregated value is disaggregated as almost zero. 
%False positive means, at time point $t_i$, 
%the real value is almost zero, 
%but the disaggregation value $\hat{y}$ is not zero. 
%False negative shows the situation that, 
%at time $t_i$, the true value $y$ is zero 
%and the disaggregated value is also zero. 


The fourth method is to evaluate by accumulating  (and comparing)
the total energy over a period of time, as in~\cite{shao2012temporal}. Once again, 
the F-measure is used to evaluate the performance of the algorithm. 

%Suppose there's ground truth time series $X$ with length T,
%and the corresponding disaggregated time series is$ X^*$.
%For any time $ t \in (0, T)$, there are two values the
%ground truth value $X_i(t)$ and the disaggregated value
%$X_i^*(t)$. We define a parameter $\rho$ as the range of
%true values $X_i(t)$ and another parameter $\theta$
%as the noise.

\subsection{Evaluation based on Combinational Metrics} 
Note that some papers propose several approaches to 
evaluate the experimental results.
Reference~\cite{liang2010load} proposes three evaluation metrics; 
detection accuracy, disaggregation accuracy, and
overall accuracy. 
The first one is the events classification accuracy.
The last two metrics are similar to the standard F-measure metric that is commonly used
in machine learning algorithms.
%? explain more

%\textbf{Summary of Evaluation Metric}
%In conclusion, the evaluation in energy disaggregation 
%is not standardized, which makes it harder to compare different works 
%even with if the same data set is used. If the research community were to agree on one or
%two evaluation metrics, a fair comparison between several algorithms can be performed. 
%Moreover, researchers can work on improving the accuracy of their algorithms for the
%%standardized evaluation metrics.
