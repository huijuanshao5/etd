\section{Identifying Devices by Algorithms}
Before applying algorithms into energy disaggregation,
pre-processing is usually needed.
It can transform data
from time domain to frequency domain
or extract features as necessary.
The algorithms applied in pre-processing
are normally different from the
disaggregation algorithm.
%some algorithms are applied.
%After pre-processing,
%other algorithms then are used to
%identify devices during disaggregation.
%For the latter part,
%All the algorithms are into three
%categories, supervised learning, unsupervised learning
%and semi-supervised learning.
In the following part of this section,
we will introduce all the algorithms
in these two consecutive steps separately.

\subsection{Pre-processing and Feature-identification Algorithms}
To extract the features or attribute of electrical devices,
some algorithms have been researched in feature selection
as shown in Table.\ref{tab_featureAlg}.
%% to be updated ???
\begin{table}[h]%
\tbl{Pre-processing Algorithms on AC-Power Feature Extraction \label{tab_featureAlg}} {%
\begin{tabular} {|l|l|l|l|l|l|l|l|l|l|}
\hline
Feature-identification Algorithm &\rotatebox{90}{real power(watts)}& \rotatebox{90} {reactive power(VARs)} & \rotatebox{90} {Startup of $I_{AC}$} & \rotatebox{90} {Harmonics of $|I_{AC}|$} &\rotatebox{90}{ $I_{AC}$}  & \rotatebox{90}{$I_{AC}$ transformation} & \rotatebox{90} {eigenvalues of $I_{AC}$} & \rotatebox{90} {$V_{AC}$} & \rotatebox{90} {voltage noise }  \\
\hline
Signal Processing\cite{cox2006transient}&&&&&&&&$\surd$& \\
\hline
Fourier Transform \cite{wichakool2009modeling}&&&&$\surd$&$\surd$&&&&\\
\hline
Goodness-of-fit\cite{jin2011robust}&&&&&$\surd$&&&$\surd$&\\
\hline
Wavelet Transforms\cite{chan2000harmonics}&&&&&$\surd$&&$\surd$&&\\
\hline
\end{tabular}}
\end{table}%

Distorted voltage shapes are compared and classified in \cite{cox2006transient} by
low-pass filter, amplification and shifting.
These approaches are often used in the area of information processing.

Switching function \cite{wichakool2009modeling} for VSDs have been described
in previous subsection.
Fourier transform of the waveform is applied to find distinct coefficients
for different devices.

\cite{jin2011robust,jin2011time} uses Goodness-of-fit (GOF) Chi-squared
to detect the transient events generated by
the first harmonics of power.
GOF exploits hypothesis approach.
At first, a change point in time series data is detected.
%\cite{jin2011robust} has good definition in window definition
%some idea: Bayesian to find the events detection instead of Chi-square tests
For $n$ independent and identically distributed(iid) data points $x_i,i=1,2,...,n$
drawn from a distribution $G(x)$ and the supposed distribution $F(x)$,
the binary hypothesis testing problem is defined as
\begin{eqnarray}
H_1: G(x) \ne F(x) \\
H_0: G(x) = F(x)
\end{eqnarray}
Then the $\chi^2$ test for goodness-of-fit(GOF) is
\begin{equation}
\mathscr{l}_{GOF} = \sum_{i=1}^{n} \frac{(y_i - x_i)^2}{x_i}
\end{equation}
When the condition in Equation.\ref{eq_chisquare} is satisfied,
e.g. with $100(1-\alpha)\%$ confidence interval,
 $H_0$ hypothesis is rejected.
\begin{equation}
\label{eq_chisquare}
\mathscr{l}_{GOF} > \chi_{\alpha,n-1}^2
\end{equation}
This means this feature can be classified into the supposed device.

High frequency data involves in harmonics, waveform and
they can be identified by signal processing algorithms.
\cite{chan2000harmonics} employs wavelet transform to
identify harmonics features of devices.
The harmonics signatures are identified accurately
by wavelet transform thus assists disaggregation
in next step.

\subsection{Overview of Disaggregation Algorithms}
As more and more features are extracted from the recorded data,
algorithms applied to disaggregation vary from simplicity
to complexity.
From the view of data mining or machine learning,
all these algorithms are sorted into three categories,
supervised, unsupervised and semi-supervised.
Table \ref{tab_algSummary} lists the categories of
all these algorithms.
\begin{table}[h]%
\tbl{Energy Disaggregation Algorithms Categories \label{tab_algSummary}} {%
\begin{tabular} {|l|l|l|l| }
\hline
Category & Algorithm Name & Example & Features Adopted  \\
\hline
\hline
\multirow{18}{*}{supervised}  & Pair-wise match & \cite{hart1992} & realpower, reactive power \\
\cline{2-4}
& rule-based & \cite{powers1991using} & real power \\
\cline{2-4}
& KNN & \cite{shaw2000PhdThesis} & startup of $I_{AC}$ \\
\cline{2-4}
& SVM & \cite{patel2007flick} & startup of $I_{AC}$ and voltage noise \\
\cline{2-4}
& SVM, AdaBoost, RBF, NN & \cite{onoda2000applying} & startup of $I_{AC}$ \\
\cline{2-4}
& SVM, KSC & \cite{onoda2000applying2} & startup of $I_{AC}$ \\
\cline{2-4}
& SVM, RBFN & \cite{nakano2007non} & real power, harmonics of $I_{AC}$ \\
\cline{2-4}
& Bayesian Classifier & \cite{berges2010enhancing} & real power \\
\cline{2-4}
& Neural Network & \cite{roos1994using} & real power, reactive power \\
\cline{2-4}
& Genetic algorithm & \cite{baranski2004genetic} & startup of $I_{AC}$, on duration \\
\cline{2-4}
& Dynamic Bayesian network & \cite{froehlich2011disaggregated} & real power \\
\cline{2-4}
& Sparse Coding & \cite{kolter2010sparse} & real power \\
\cline{2-4}
& Decision Tree & \cite{berges2009learning} & startup of $I_{AC}$ \\
\cline{2-4}
& AdaBoost & \cite{berges2009learning} & startup of $I_{AC}$ \\
\cline{2-4}
& General likelihood ratio & \cite{anderson2012event} & real power, reactive power, on/off duration \\
\cline{2-4}
& Duration PDF & \cite{zeifman2011viterbi} & real power, on duration \\
\cline{2-4}
& Integer Programming & \cite{suzuki2008nonintrusive} & $I_{AC}$ \\
\cline{2-4}
& Dynamic Programming & \cite{baranski2004detecting} & $I_{AC}$ \\
\hline
\hline
\multirow{4}{*}{unsupervised}  & Hierarchical Clustering & \cite{lam2007novel} & startup of $I_{AC}$, $I_{AC}$, $V_{AC}$ \\
\cline{2-4}
& Factorial HMM & \cite{kim2010unsupervised} & real power, on/off duration, time, correlation \\
\cline{2-4}
& AFAMAP & \cite{kolter2012aistat} & real power, startup of $I_{AC}$ \\
\cline{2-4}
& Motif Mining & \cite{shao2012temporal} & real power \\
\hline
\hline
semi-supervised & Variant HMM & \cite{parson2012nonintrusive}& real power \\
\hline
\end{tabular}}
\end{table}

%So far, data mining algorithms focus on the
%low sampling frequency recorded data set.
%Optimization algorithms is employed on both high frequency
%and low frequency data set.
%Signal processing algorithms are mainly applied to
%the high sampling frequency recorded data.
%Fig.\ref{fig_algFlowchart} describes the relationship of
%these three types of algorithms.
%Signal processing algorithms can run stand-alone to identify devices.
%Also, it can be integrated with data mining or optimization
%algorithms as the preprocessing part.
%Under the latter condition, usually the data is
%transformed from the time domain to frequency domain.
%\begin{figure}[ht]
%\centering
%\includegraphics[width=4in]{figs/algorithmsFlowchart.pdf}
%\caption{Algorithms Flowchart}
%\label{fig_algFlowchart}
%\end{figure}
According to the events produced by electrical devices,
all algorithms are classified into event-based algorithms
and non-event based algorithms.
Even-based algorithms dedicate to processing the on and off
events.
Non-event based algorithms treats the current, voltage or
power value as ordered time series instead of
picking up the transition states.
Even-based algorithms performs well on discrete steady-states
devices.
By comparison for those variable speed devices (VSDs),
event-based algorithms take no effect.

To have a look at the algorithms and the features in detail,
the major algorithms and the adopted features of each algorithm
are listed in Table.\ref{tab_algorithms} in Appendix section.

\subsection{Supervised Learning Algorithms}
Supervised learning on energy disaggregation is
to train a model with each circuit/device's recorded
data and the aggregated data over a period of time,
then distinguish all devices from the
aggregated data given another period's data.
The exploration of supervised learning
on energy disaggregation has been developing
since this topic was proposed.
A lot of experience has accumulated in the area of
this type of approaches.
Some classification algorithms are used alone,
others are combination of several classification algorithms.
These classification algorithms include
simple pair-wise match, rule-based algorithm,
KNN, SVM, Kernel Based Subspace Classification,
Bayesian classifier, neural network,
genetic algorithm,
dynamic bayesian network,
sparse coding,
AdaBoost,
decision tree,
a combination of SVM and AdaBoost
and etc.

\subsubsection{K-Nearest Neighbor}
Fig.\ref{fig_transientClassifier} in \cite{shaw2000PhdThesis} describes how
transient spectral envelopes of devices are discovered.
There are exemplar transient sections for each device recorded in
T1 and T2.
As the data flow comes from right to the left,
firstly the data flow is preprocessed whether
there's an section event occurs in the width of 1000
data points.
If there's an event, the identified event section is compared
with all the pre-stored exemplar transient spectral envelops.
This section is classified into that transient spectral envelop
with least square distance.
%The transient spectral envelopes are classified
%based on the least square distance between
%the aggregated time series and exemplar transient spectral envelopes.
\input{figs/shaw2000_transientClassifier.tex}

The comparison of this approach just makes use of the
Euclidean distance to measure the similarity of
identify events with the pre-defined spectral envelop
of device.

In order to identify variable speed devices (VSDs),
\cite{lee2005estimation} builds a table to stores
the real power, reactive power and harmonics for each device.
Then, the signatures extracted from the aggregated power
are compared with the stored features in the table.
The disaggregated signature belongs to the device,
whose feature is most similar to the feature stored
in the table.
This process is exactly what K-nearest neighbor does.
Therefore \cite{lee2005estimation} is classified into
KNN category.

\cite{shaw2008nonintrusive} utilized the real power
and transient shapes to identify devices.
The transient signature,
which is also called exemplar,
is composed of two vectors,
shape vector and time vector.
Therefore when comparing the transient shapes,
some characteristic parts are needed
rather than uninterrupted whole transient shapes.
Fig.\ref{fig_shaw2008_transient} depicts an exemplar
with two shape vectors $s_1$ and $s_2$.
For this transient exemplar,
there's also an overall scale factor $\alpha$ and
time shift $k$ and offset $b_k$.
To identify which device the disaggregate signature belongs to,
the disaggregation feature is to compare with this exemplar
by least square criteria.
\begin{figure}[ht]
\centering
\includegraphics[width=3 in]{figs/shaw2008non_transient.pdf}
\caption{Transient Vectors (courtesy: \cite{shaw2008nonintrusive}).}
\label{fig_shaw2008_transient}
\end{figure}
Although this paper doesn't mention KNN algorithm,
the above description is to figure out the closest shapes,
e.g. KNN.

\cite{berges2009learning} and \cite{berges2010enhancing} makes use of
K-Nearest Neighbor (K=1)
to classify the ON/OFF events created by all devices.
In the experiment, refrigerator is tested by comparing with the plug-level
data which is installed to monitor the monitor.
Event classification error rate is the evaluation metric.
The experimental results show that other two devices with similar power levels of
refrigerator increase the predicting error rate of events produced
by refrigerator.

\cite{gupta2010electrisense} employs KNN to identify devices
with switch mode power supplies (SMPS).
SMPS generate high frequency interference (EMI) when it's turned on or off.
This paper builds a system named ElectriSense.
Initially, the baseline of power lines are recorded as historical data
during the training process.
25 frequency vectors are stored as the
baseline noise signatures when the system starts.
Then a global threshold 8dB is set as the power
threshold above noise baseline.
After a newly event is added on a particular power line,
KNN is exploited to
decide which vector of this events should be classified
over these 25 events.
In this step, a window is set to match the difference vector,
e.g. the distance between the vector with newly-added event and
baseline noise vector.
Once there's peak above the pre-defined threshold,
a Gaussian function is applied to calculate
the mean value, standard deviation.
Last during the KNN calculation,
Euclidean distance metric
with inverse weighting is applied to classify
the difference vector.
%\cite{berges2009learning} tests with four approaches,
%K-Nearest Neighbor, Gaussian Naive Bayes(GNB),
%Decision Trees(DT) and Multiclass Adaboost(MultiBoost).

\subsubsection{Support Vector Machine}

\cite{onoda2000applying} focus on the inverter and non-inverter devices.
Devices with inverter system include refrigerator, vacuum cleaners,
microwave ovens and so on.
These devices are identified by SVM with harmonics features
generated by ON/OFF events.

Fig.\ref{fig_inverterDevices} illustrates the power curve of
refrigerator with and without inverter system.
Without inverter system, the refrigerator just repeats the
ON/OFF pattern frequently. With inverter system,
refrigerator works in another work mode as the upper figure.
\begin{figure}[ht]
\centering
\includegraphics[width=3 in]{figs/inverterDevices.png}
\caption{Refrigerator w/ and w/o Inverter System (Courtesy:\cite{onoda2000applying})}
\label{fig_inverterDevices}
\end{figure}
Support vector machine is to find a large margin between
two classes thus minimize the distances of each plate.
Fig.\ref{fig_svmClassifier} shows the work flow to classify ON/OFF events
for different devices.
\begin{figure}[ht]
\centering
\includegraphics[width=3 in]{figs/svmClassifier.png}
\caption{SVM Classifier for Devices (Courtesy:\cite{onoda2000applying})}
\label{fig_svmClassifier}
\end{figure}

SVM is also adopted by \cite{patel2007flick}
to classify transient pulses noise
from various homes.
SVM is employed because the data recorded based on noise feature
is larger than that of AC power feature and SVM works fast on
large data set.
During processing, firstly transient events are isolated and
then the disaggregated events are classified into these transient events.
The test is done for the residential buildings.
The author figures out it could be applied to commercial buildings
which may include more compound devices and complex noises.

\cite{nakano2007non} employs support vector machine to classify the
harmonics of each devices during on and off events.
Both stand-alone SVM and combination of SVM and radial basis function network (RBFN)
are implemented to compare the disaggregated data with the ground truth harmonics.

SVM is applied to transient and continuous voltage noise data as stated in \cite{froehlich2011disaggregated}.
Noise voltage is produced by devices thus influences
the power wiring.
According to Gen Marubayashi \cite{mambayashi1997noise},
there are three types of voltage noise,
namely on-off transient noise,
steady-state line voltage noise which is produced at $60Hz$ or
integer times of $60Hz$ (e.g. harmonics),
and steady-state continuous noise which is generated beyond
$60Hz$.
Voltage noise data are sampled by very high frequency.
Firstly, these recorded voltage noise data
are transformed by Fourier analysis.
Initially, three to five transient voltage noise signatures are labeled
and a threshold is pre-defined.
During training procedure,
by sliding a window on the aggregated voltage noise,
a part of data of continuous voltage noise data
is extracted and compared with the pre-stored voltage noise data by
Euclidean distance.
If the distance is larger than the pre-defined
threshold, then the feature vector exerted from
this window.
After sliding over aggregated voltage noise data,
all these feature vectors are classified
by SVM.

\subsubsection{Kernel Based Subspace Classification}
\cite{onoda2000applying2} uses a kernel based subspace classification(KSC)
approach for events classification.
A RBF kernel is introduced as Equation.\ref{eqn_rbfSubspace} for both KSC and SVM.
\begin{equation}
\label{eqn_rbfSubspace}
k(x,y)=exp(\frac{{\lVert x-y \rVert}^2}{\sigma ^2})
\end{equation}
Where $x$ is the dimensional patterns,
$y$ represents the classified devices,
$k(x, y)$ is the kernel matrix and
its $(i, j)$ element is $k(x_i, y_j)$.

Then parameters for KSC and SVM are estimated and tested.
The results shows that KSC performs with lower error rate
for some devices and a bit higher error rate for other devices.
However, the computation cost of KSC is much lower than SVM.

\subsubsection{Bayes Classifier}
Besides aforementioned KNN,
\cite{berges2010enhancing} also employs Gaussian Bayes Classifier
to classify the ON/OFF events.

\subsubsection{Neural Network}
Neural network is adopted by several papers in energy disaggregation area
It makes uses of one or $N$ features as the input,
after defining $K$ hidden states,
then we can get the $M$ output devices
as shown in Fig.\ref{fig_combinedNN_chang2008load2}.
\begin{figure}[ht]
\centering
\includegraphics[width=3 in]{figs/combinedNN_chang2008load2.pdf}
\caption{Neural Network Approach for Energy Disaggregation.}
\label{fig_combinedNN_chang2008load2}
\end{figure}
Neural network has the advantage over detect the interaction between
the disaggregated features and ground truth
but it works very slow.

\cite{roos1994using} initially proposed to adopt
neural network for classification based on real
power and reactive power feature.
It transforms these information into images for processing.
\cite{baranski2003nonintrusive} employs back propagation
neural networks with attributes of states number,
duration time and average energy consumption to identify
devices.

Both \cite{duan2004neural} and \cite{srinivasan2006neural} trains based on the current waveform
and harmonics to extract signatures thus identify devices.
Besides neural network, \cite{srinivasan2006neural} paper compares several approaches,
multilayer perception(MLP), radial basis function(RBF) network,
support vector machine(SVM) with linear and polynomial and
RBF kernels.
The system work flow is Fig.\ref{fig_system_srinivasan2006neural}.
In the first step, the distorted waveform generated by non-linear
devices is transformed by Fourier analysis.
\begin{figure}[ht]
\centering
\includegraphics[width=3 in]{figs/system_srinivasan2006neural.png}
\caption{System for Neural Network (Courtesy:\cite{srinivasan2006neural})}
\label{fig_system_srinivasan2006neural}
\end{figure}
Then the eight odd-numbered harmonics are extracted as vectors as
illustrated in Fig.\ref{fig_harmonics_srinivasan2006neural}.
\begin{figure}[ht]
\centering
\includegraphics[width=3 in]{figs/harmonics_srinivasan2006neural.png}
\caption{Harmonics Signatures(Courtesy:\cite{srinivasan2006neural})}
\label{fig_harmonics_srinivasan2006neural}
\end{figure}
The real part and imaginary part are separated as the input of
ANN.
\begin{eqnarray*}
x_i=I_{(i+1)/2}cos\phi_{(i+1)/2}, for \quad i=1, 3, 5, 7, 9, 11, 13,\quad and\quad 15 \\
x_i=I_{i/2}sin\phi_{i/2}, for \quad i=2, 4, 6, 8, 10, 12, 14\quad and\quad16
\end{eqnarray*}
Where $I_n$ is the magnitude of the $nth$ odd current harmonics and
$\phi_n$ is the phase angle of the $nth$ odd current harmonics.
Since there are 8 devices, the output number of nodes is 8.
The number of hidden nodes is chosen as 16.
The results indicate that MLP and RBF-based approach performances with
high classification rate.

\cite{yang2007design} and \cite{chang2008load} classify the transient events by way of
back propagation (BP) and learning vector quantization (LVQ) to
recognize devices.

In \cite{chang2008load2}, Chang et al. extends the work
to employ back propagation by electromagnetic transient program (EMTP) with
real and reactive power feature.
A window size $\Delta t $ is adopted to enhance the algorithm
as adaptive neural network. %as Fig.\ref{fig_nnsystem_chang2008load2}
Initially, the differential values $dW_{transient}$ for a period time $\Delta t$ represent the characteristics of a class of devices. During the training process,
the time period value $\Delta t$ increases by $\delta$ from $1$ to $\Delta t$,
the $\delta$ which achieves highest recognition accuracy is kept.
In this procedure, $\delta$ changes adaptively.
\begin{eqnarray*}
W_{transient} = \int_{t_s}^{t_s+\Delta t} v\cdot i \cdot dt \\
P_{instantaneous}(t) = \frac{dW_{transient(t)}}{dt} = v(t)\times i(t)
\end{eqnarray*}
\input{figs/nnsystem_chang2008load2.tex}

\cite{liang2010load} selects Back propagation ANN (BPANN) to
train the model which leads to high accuracy.
Many kinds of feature are gathered together, e.g. real, reactive power,
transient shapes, harmonics, eigenvalue of current waveform,
voltage waveform and so on.
Further, \cite{liang2010load} establishes a committee decision system.
Three rules, most common occurrence(MCO), least unified residue(LUR) and
maximum-likelihood estimation(MLE) are employed to classify devices.

\cite{chang2010newmethod} combines neural network and genetic algorithm together
on the turn-on transient signatures.
The approach of multi-layer feed-forward neural network is employed.

%neural networks\citeNP{baranski2003nonintrusive,duan2004neural,srinivasan2006neural,yang2007design,chang2008load,liang2010load,chang2010newmethod,chang2008load2} and

\subsubsection{Genetic Algorithm}
Genetic algorithm is proposed by \cite{baranski2004genetic}
and its framework shows
as Fig.\ref{fig_alg_baranski2004genetic}.
\begin{figure}[ht]
\centering
\includegraphics[width=5 in]{figs/alg_baranski2004genetic.pdf}
\caption{Combined Neural Network (Courtesy:\cite{baranski2004genetic})}
\label{fig_alg_baranski2004genetic}
\end{figure}
This approach treats multiple power-level devices as finite state machine
then employs the real power characteristics to identify devices.
The results of fuzz clustering is the input of genetic algorithm as Fig.\ref{fig_genetic_baranski2004genetic}.
\input{genetic_baranski2004genetic.tex}

Then dynamic programming to discover the shortest path as finite state machine.
All the ON/OFF events are shown in the time series as $\Delta P_i = P_i - P_{i-1}$.
To evaluate whether the shortest path $\Gamma_l={Z_{l1},...,Z_{lk}}$,
 which represents the
finite state machine is proper or not,
a quality criterion which is similar to Shannons entropy is measured.
\begin{equation}
Q_l=  - \sum_i \Delta e_i log|\Delta e_i|
\end{equation}
and
\begin{equation}
\Delta e_i = |\frac{\Upsilon_i(\Gamma_l)-\Upsilon_i(\Gamma_c) }{\Upsilon_i(\Gamma_c)}|+e_0
\end{equation}
Here $\Upsilon_i$ represents ON duration or real power standard deviation of
the state $Z_i$.
Last, power levels such as (150W, 50W, -200W) can be found.

In another paper \cite{chang2010newmethod},
genetic programming is integrated with neural network
to identify devices.

\subsubsection{Dynamic Bayesian Network}
Combination of SVM and Dynamic Bayesian Network is employed by \cite{froehlich2011disaggregated}.
Initially a threshold to measure the Euclidean distance between new data and
basic noise data is predefined. Then a windows slides on to find whether the distance
exceeds the threshold hold. According to Euclidean distance,
the feature vectors which denotes devices are classified by SVM.
Dynamic Bayesian Network can exploited for the devices with
prior information, such as washing machine, dryers and
HVAC as mentioned in the future work part.

\subsubsection{Sparse Coding}
\cite{kolter2010sparse} introduces non-negative sparse coding to solve the energy disaggregation problem
as
\begin{equation}
\min_{A_i\geq0,B_i\geq0}\frac{1}{2}\lVert X_i- B_iB_i\rVert_F^2 + \lambda\sum_{p,q}E(A_i)_{pq}
subject \qquad to \qquad \lVert B_i^{(j)} \rVert_2 \leq 1, j=1,...,n
\end{equation}
The sparse coding targets to learn basic function for each class instead of minimizing
the disaggregation error.
Based upon structured prediction methods,
a regularized disaggregation error is defined.
\begin{equation}
\label{eq_regError_kolterNips}
E_{reg}(X_{1:k},B_{1:k}) \equiv E(X_{1:k},B_{1:k})+ \lambda \sum_{i,p,q}(\hat{A}_i)_pq
\end{equation}
Then a sparse set of coefficients are iteratively calculated by
minimizing this regularized disaggregation error as Equation.\ref{eq_regError_kolterNips}.
The best solution for $\hat{A}_i$ becomes
\begin{equation}
A_i^\star = arg \min_{A_i\geq0} \frac{1}{2} \lVert X_i-B_iA_i \rVert_F^2 + \lambda \sum_{p,q}(A_i)_pq
\end{equation}
where $X_i$ is the data matrix.
In order to get the best value of $A_{1:k}^\star$,
$B_{1:k}$ is optimized in each iteration during
discriminatively training phase.
Then in the same iteration,
the base of $B_{1:k}$ is updated to calculate $\hat(A)_{1:k}$ again.
By updating $A_{1:k}^\star$ and $B_{1:k}$ alternatively,
the real power of each device can be predicted by Equation.\ref{eq_results_kolterNips}.
\begin{eqnarray*}
\label{eq_results_kolterNips}
%$\hat{A}_{1:k}^{\prime}= arg\min_{A_{1:k}\geq 0 } F(X, )
\hat{X}_i^{\prime}= B_i\hat{A}_i^{\prime}
\end{eqnarray*}


%The detailed algorithm is describe in Fig.\ref{fig_DDSC_kolter2010sparse}
%\begin{figure}[ht]
%\centering
%\includegraphics[width=5in]{figs/DDSC_kolter2010sparse}
%\caption{Discriminative Disaggregation Sparse Coding:\cite{kolter2010sparse})}
%\label{fig_DDSC_kolter2010sparse}
%\end{figure}
%\begin{equation}
%\tilde{E}_{reg}(X_{1:k},B_{1:k},\tilde{B}_{1:k}) \equiv \sum_{i=1}^{k}()
%\end{equation}

\subsubsection{AdaBoost, Decision Tree}
\cite{berges2009learning} tests with four approaches,
K-Nearest Neighbor, Gaussian Naive Bayes(GNB),
Decision Trees(DT) and Multiclass Adaboost(MultiBoost)
for the high frequency data of time.

\cite{onoda2000applying} integrates SVM with Adaboost to classify devices
based on odd-number harmonics.
In this case, Adaboost helps SVM to classify those unclear points.
Regarding support vector machine,
the margin $Q$ is defined as
\begin{equation}
Q= \min_{i=1,...,l}\rho(z_i,f)
\end{equation}
where
\begin{equation}
\rho(z_i,f)= y_i f(x_i)
\end{equation}
AdaBoost is to minimize the margin
$\rho(Z_i,\alpha):=\rho(Z_i, f_\alpha)$ on the training set
\begin{equation}
\mathscr{G}(a)= \sum_{i=1}^l exp\{-\lVert \alpha \rVert_1 (\rho(z_i, \alpha)-\phi)\}
\end{equation}
To achieve this goal, every example $Z_i$ is
given a weight $w^t(Z_i)$.
Applying bootstrap on the weighted sample distribution,
we can find $\alpha_t$ to minimize $\mathscr{G}(\alpha)$.

\subsubsection{Closure Rules}
Closure-rules with maximal length of four is used \cite{hampden2012closure}
on the feature of real and transition states.
The principle of closure rules is that
if only one device change its state,
the baseline signature is the same before this state
change event occurs.
Fig.\ref{fig_hampden2012closure} illustrates an example.
Circles represent steady states and arrows represent transitions.
\begin{figure}[ht]
\label{fig_hampden2012closure}
\centering
\includegraphics[width=3in]{figs/rules_hampden2012closure.png}
\caption{Example of more complex closure map from real measurements (Courtesy: \cite{hampden2012closure}).}
\label{fig_rules_hampden2012closure}
\end{figure}

\subsubsection{Integer Programming}
Integer programming \cite{suzuki2008nonintrusive} is applied
to the current waveform as supervised learning.
Each device's waveform is stored in the database,
then disaggregation process goes to identify the
devices according to the pre-stored each device's information.
Suppose there is aggregated load current $\hat{i}$,
\begin{equation}
\hat{i}(t) = \sum_{n=1}^{N}c_n(m)i_n(t)+\epsilon
\end{equation}
where $c_n \in \{0,...,C_n\}$ for $n\in{1,...,N}$
$N$ is the device number.
To estimate $c_n$ from the aggregated $\hat{i}_(t)$,
then this problem is abstracted as an integer
quadratic programming problem
\begin{equation}
\min \sum_{t=0}^{T-1}(\hat{i}(t)- \sum_{n=1}^{N}c_n(m)i_n(t))^2
\end{equation}
subject to
\begin{displaymath}
c_n \in \mathcal{Z}, 0 \leq c_n \leq C_n \forall n \in \{1,...,n \}.
\end{displaymath}

\subsubsection{Duration PDF}
\cite{zeifman2011viterbi} describes to apply viterbi algorithm to
devices with several two-states with Probability Density Function of
the real power.
The negative values are clustered firstly and the $ith$ cluster
represents device $i$.
Then the positive values are clustered to match the negative clusters.
The only possibility for the device $i$ overlap is with device $i-1$ and $i+1$.
Fig.\ref{fig_neighbour_zeifman2011viterbi} illustrates three PDF with
close real power that may overlap.
\begin{figure}[ht]
\centering
\includegraphics[width=3in]{figs/neighbour_zeifman2011viterbi.png}
\caption{PDFs of three neighboring by power draw appliances. Distribution
means (circles) are located at a distance of 4 standard deviations apart. \cite{zeifman2011viterbi}}
\label{fig_neighbour_zeifman2011viterbi}
\end{figure}
Hence, to separate devices from the PDF,
only the PDF relationship of $(i-1, i)$ and
$(i, i+1)$ need to be considered.
The Viterbi-type algorithm can greatly
decrease the computational cost compared
to brute-force Viterbi algorithm.
Its computation complexity is linear
to the number of devices.

Another paper extends PDF by a conjunction of Semi-Markov and Viterbi-type algorithm algorithm\cite{zeifman2012disaggregation}
is offered for distinguishing electricity without training.
Fig.\ref{fig_pdf_ziefman2012} shows a device with many power levels which
overlap with five devices.
\begin{figure}[ht]
\centering
\includegraphics[width=3in]{figs/pdf_ziefman2012.png}
\caption{Possible PDFs of power draw of appliances i and its neighbors (courtesy: \cite{zeifman2012disaggregation})}
\label{fig_pdf_ziefman2012}
\end{figure}
The standard Viterbi algorithm is to maximize the likelihood
\begin{equation}
\{\hat{S}_t \} = {argmax}_{s(t)}[\{S_t\}|\{\omega_t\}]
\end{equation}
Where $\{S_t\}$ is the state sequence and
$\{\omega_t\}$ is the transition observations.

\subsubsection{Dynamic Programming}
\cite{baranski2004detecting} employs mathematical dynamic programming
with genetic algorithm to find the multiple-state devices that can be represented as
finite state machine(FSM).

\subsubsection{General Likelihood Ratio(GLR)}
\cite{anderson2012event}, \cite{berges2011user} and \cite{luo2002monitoring}
uses the general likelihood ratio to classify
the events generated by different devices.

%\subsubsection{Hybrid Algorithms}

\subsection{Unsupervised Approaches}
When Hart initially propose energy disaggregation,
the basic target is for unsupervised learning in the
data mining area\cite{hart1992} because
the exact information of individual circuit or device
is unknown.
Recent research gradually convert to unsupervised learning.

\subsubsection{Hierarchical Clustering}
In \cite{lam2007novel}, initially shape features voltage-current (V-I) trajectory of appliances are extracted.
Then hierarchical clustering are exploited to cluster the appliances by
analyzing these V-I trajectory.
During hierarchical clustering,
the pair distances among shape features
are calculated.
Then a dendrogram is created to show the
relationship among devices like Fig.\ref{fig_dendrogram_lam2007novel}.
\begin{figure}[ht]
\centering
\includegraphics[width=5 in]{figs/dendrogram_lam2007novel.png}
\caption{The dendrogram for clustering the appliances. The upper
horizontal dotted line divides the appliances into 13 groups. Three groups
are further divided into subgroups by the three lower dotted lines.\cite{lam2007novel})}
\label{fig_dendrogram_lam2007novel}
\end{figure}
%Hierarchical clustering use V-I trajectory as described in \cite{lam2007novel}.
%The purpose of installing meters on each circuits is
%for evaluation instead of training the model.

%\subsubsection{Blind Source Separation}
%Real and reactive power is adopted as the features.
%Blind source separation is initially used in the area of speech recognition.
Hierarchical agglomerative Clustering of real and reactive power \cite{gonccalves2011unsupervised} is used to cluster the ON/OFF events.
The greedy matching pursuit(MP),
which is a direct implementation of Hart's thoughts,
is employed to calculate in terms of Euclidian distance
$([P_{obs},Q_{obs}]-[P_{closest},Q_{closest}])$.

\subsubsection{Factorial HMM}
\cite{kim2010unsupervised} proposes to employ
Factorial Hidden Markov Model(FHMM) and
semi-FHMM to disaggregate devices.
Factorial Hidden Markov Model, shown as in Fig.\ref{fig_fhmm_kim2010unsupervised}
uses multiple HMMs to model
the status of each device and the sum of power values
is the aggregated power value.
\begin{figure}[ht]
\centering
\includegraphics[width=3in]{figs/fhmm_kim2010unsupervised.pdf}
\caption{Graph Model of FHMM with $N$ Devices.}
\label{fig_fhmm_kim2010unsupervised}
\end{figure}
By introducing the on/off duration,
correlation of devices and
usage time of devices,
semi-FHMM and constraint semi-FHMM
are generated as Fig.\ref{fig_CFSHMM_kim2010unsupervised}
\begin{figure}[ht]
\centering
\includegraphics[width=3in]{figs/CFSHMM_kim2010unsupervised.png}
\caption{Graph representation of CFSHMM (courtesy: \cite{kim2010unsupervised}).}
\label{fig_CFSHMM_kim2010unsupervised}
\end{figure}
By calculating the initial probability $\phi_{in}(Y,q|\lambda)$,
emission probability $\phi_{e}(Y,q|\lambda)$ and
transition probability $\phi_{t}(Y,q|\lambda)$,
where $\lambda$ is the parameters set.
The product of these three probability is as Equation.\ref{eq_fhmm}
\begin{equation}
\label{eq_fhmm}
P(Y,q|\lambda)= \phi_{in}(Y,q|\lambda) \cdot \phi_{e}(Y,q|\lambda) \cdot \phi_{t}(Y,q|\lambda)
\end{equation}
By maximizing Equation.\ref{eq_fhmm_em} with EM algorithm,
the HMM which represents device can be recovered.
\begin{equation}
\label{eq_fhmm_em}
\phi(\lambda,\lambda^\prime)= \sum_q P(Y,q|\lambda^\prime) log P(Y,q|\lambda)
\end{equation}
where $\lambda^\prime $ and $\lambda$ represents the previous and current
iteration parameters set of EM algorithm.

\subsubsection{AFAMAP}
Then Kolter \cite{kolter2012aistat} extended this model to
Additive Factorial Approximate Maximize a Posterior(MAP),
which is the mixture of  additive factorial model and
difference FHMM model.
The box diagram of AFAMAP is as Fig.\ref{fig_AFAMAP_boxDiagram}
\begin{figure}[ht]
\centering
\includegraphics[width=3 in]{figs/AFAMAP_boxDiagram.pdf}
\caption{AFAMAP Flowchart}
\label{fig_AFAMAP_boxDiagram}
\end{figure}
Initially, the Maximize A Posterior(MAP) is proposed and
priors are defined as \ref{eq_afamapPrior}.
\begin{eqnarray}
\label{eq_afamapPrior}
p(z_{1:T})= \frac{1}{Z(\lambda,T)}exp{-\lambda \sum_{t=1}^{T-1} \lVert z_{t+1}-z_{t-1}\rVert_1}
p(\Delta z{1:T})=\frac{1}{Z(\lambda,T)}exp{-\lambda \sum_{t=1}^{T}\lVert \Delta Z_t \rVert_1}
\end{eqnarray}
Thus the posterior of additive and additive model becomes
\begin{displaymath}
\bar{y}_t|x_t^{1:N},z_t\sim N(\sum_{i=1}^{N}\mu_{x_t^{(i)}}^{(i)}+\Sigma_{z_t}^{1/2},\Sigma)\\
\Delta\bar{y}_t|x_{t-1}^{1:N},\Delta z_t\sim N(\sum_{i=1}^{N}\Delta \mu_{x_t^{(i)},x_{t-1}^{(i)}}+\Sigma^{1/2}\Delta z_t,\Sigma)\\
\end{displaymath}
Then the once-at-a-time constraints was added as Equation.\ref{eq_onceatime}
\begin{equation}
\label{eq_onceatime}
\mathcal{O} = {\mathcal{Q}: \sum_{i,j,k \neq j} Q(x_{t-1}^{(i)},x_t^{(i)})_{j,k}\leq 1}
\end{equation}
and
Huber loss function is employed for optimization.
\begin{eqnarray}
D(y,\lambda)&=& \min_{z}\{\lVert y-z \rVert_2^2+ \lambda \lVert z \rVert_1\} \\
&=& \sum_{\ell=1}^{n}\min{\frac{1}{2}y_{\ell}^2,max{\lambda|y_{\ell}|-\frac{\lambda^2}{2}, \frac{_\lambda^2}{2}}}
\end{eqnarray}

Thus disggregation is converted to minimize Equation.\ref{eq_afamapMin}.
\begin{equation}
\label{eq_afamapMin}
...
\end{equation}
For this algorithm,
the input is aggregated power,
the $N$ number of HMM are output,
which corresponds to $N$ devices.
%The AFAMAP algorithm is as follows\ref{fig_afamapAlg_kolter-aistats12}.
%\begin{figure}[ht]
%\centering
%\includegraphics[width=3 in]{figs/afamapAlg_kolter-aistats12.png}
%\caption{AFAMAP Algorithm \cite{kolter2012aistat}}
%\label{fig_afamapAlg_kolter-aistats12}
%\end{figure}

\subsubsection{Motif Mining}

A lightweight time series motif mining method \cite{shao2012temporal}
is proposed to identify those devices rapidly.
In this approach, motif which represents a multiple-states
of a device is supposed to discovered in the time series.
Fig.\ref{fig_motifSample} illustrates how a motif is found.
Non-overlap search for a single episode explains a
multiple-states changes for a device. A device turns on, then
its state changes to other state, until turns off. This episode
corresponds to a complete running cycle of a device. A
device may include multiple episodes.
Between any two episodes, overlap does exist. For example,
the second instance of Episode1 overlaps with the first
instance of Episode 2. The overlap between episodes explains
the operations of several devices. We regard Episode
1 as a device A and Episode 2 as a device B. When device
B turns on for the first time, before it turns off, device A
turns on for the second time then turns off, then device B
turns off.
\begin{figure}[ht]
\centering
\includegraphics[width=3in]{figs/motifSample.png}
\caption{Motif Mining Example (\cite{shao2012temporal}). }
\label{fig_motifSample}
\end{figure}
Also, it can integrate with AFAMAP\cite{kolter2012aistat}.
The output of motif mining can be as the input of
AFAMAP.

\subsection{Semi-supervised Algorithms}
%\cite{bellala2011towards} utilizes a semi-supervised approach on dataset from commercial buildings.
Repeatedly deleting the identified device\cite{parson2012nonintrusive}
provides a different thought on disaggregation.
To disaggregate it, a variant HMM is implemented.
The prior knowledge of appliance behavior, e.g. the peaks when
a device powers on, and power demand was employed.
\begin{figure}[ht]
\centering
\includegraphics[width=3in]{figs/hmm_parson2012nonintrusive.png}
\caption{A difference HMM variant where observation $y_3$
shown by dashed lines has been filtered out (courtesy: \cite{parson2012nonintrusive}).}
\label{fig_parson2012nonintrusive}
\end{figure}
Then EM algorithm runs to judge whether the likelihood that
 it's a device or not.
\begin{displaymath}
accept(x_i,...,x_j|\hat{\theta})= \left \{ \begin{array} {ll}
true & \textrm{if $Ln\mathcal{L} > \mathcal{D}$ } \\
false & \textrm{otherwise} \end{array} \right.
\end{displaymath}
where $x_i,...,x_j$ represents the data in a window size
begins from index $i$ to $j$, $\mathcal{L}$ notes the
likelihood given the prior parameter $\hat{\theta}$,
$\mathcal{D}$ is the predefined likelihood threshold.
Since this paper chooses a part data for training to
get the prior information of devices,
it's classified into semi-supervised algorithm.

viterbi algorithm can be used standalone\cite{hart1992} or
integrated with HMM[\citeNP{zeifman2011viterbi}].


%\subsection{Non-event and Event-based}
%For these algorithms, some are related to temporal time series,
%some are only based on the events.
%Time series algorithms usually relates to the transient events or high
%frequency data involving in time and frequency.
%
%The non-event algorithms, as a function of time,
%include \citeNP{shaw2000PhdThesis,onoda2000applying,baranski2003nonintrusive,baranski2004genetic,patel2007flick,yang2007design,lam2007novel,chang2008load,chang2008load2,berges2009learning,kim2010unsupervised,liang2010load,chang2010newmethod,froehlich2011disaggregated,zeifman2011viterbi,kolter2012aistat,zeifman2012disaggregation,shao2012temporal,parson2012nonintrusive}.
%
%A variance of temporal mining is after the Fourier transform, as a function of frequency, e.g. harmonics,
%in the frequency domain \citeNP{duan2004neural,srinivasan2006neural,onoda2000applying}.
%
%The event-based algorithms include \citeNP{hart1992,roos1994using,nakano2007non,suzuki2008nonintrusive,gupta2010electrisense,berges2010enhancing,kolter2010sparse,gonccalves2011unsupervised,hampden2012closure}


%Temporal mining, \cite{baranski2004genetic} performs similar to motif mining,
%closure rules algorithm.
